<!DOCTYPE html>
<html lang="ja">
<head>
  
    <title>[論文読み] Neural Inverse Rendering of an Indoor Scene From a Single Image :: #a5ebec — my scribbled blog</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content="Neural Inverse Rendering of an Indoor Scene from a Single Image Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, Jan Kautz CVPR, 2019 arXiv, SemanticScholar  What it is about 単一画像からの屋内シーンのニューラルインバースレンダリング
インバースレンダリングとは 画像からシーンの物理的属性
 物体形状(表面法線ベクトル) 反射特性(アルベド) 光源分布(照明マップ)  を推定することを目的としている．
Why it is worthy researching 屋内シーンの単一画像を，Inverse Rendering Network (IRN)を用いて
 アルベド 表面法線ベクトル 照明の環境マップ  の3つの属性に分解する．
今までの手法では，主に単一のオブジェクトに対して，またはシーン属性の１つのみを解決するものだった．
本稿では，屋内シーンの単一画像に対してそれらのシーン属性を同時に解くことができる事ができる．
また，SUNCG-PBRという名のデータセットを作成している．
このデータセットは以前のデータセットを大幅に改善したもの
 鏡面反射を仮定したシーン 拡散反射を仮定したシーン ground truth depth surface normals albedo Phong model parameters semantic segmentation glossiness segmentation  以前のデータセットと比べてより写実的でノイズが少ないのが特徴"/>
<meta name="keywords" content="blog"/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="https://blog.5ebec.dev/posts/neural-inverse-rendering-of-an-indoor-scene-from-a-single-image/" />


<link rel="stylesheet" href="https://blog.5ebec.dev/assets/style.css">

  <link rel="stylesheet" href="https://blog.5ebec.dev/assets/a5ebec.css">



<link rel="stylesheet" href="https://blog.5ebec.dev/style.css">


<link rel="apple-touch-icon-precomposed" sizes="180x180" href="https://blog.5ebec.dev/img/favicon/apple-touch-icon-180x180-precomposed.png">

<link rel="shortcut icon" href="https://blog.5ebec.dev/img/favicon/favicon.ico">



<meta property="og:locale" content="ja" />
<meta property="og:type" content="article" />
<meta property="og:title" content="[論文読み] Neural Inverse Rendering of an Indoor Scene From a Single Image :: #a5ebec">
<meta property="og:description" content="Neural Inverse Rendering of an Indoor Scene from a Single Image Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, Jan Kautz CVPR, 2019 arXiv, SemanticScholar  What it is about 単一画像からの屋内シーンのニューラルインバースレンダリング
インバースレンダリングとは 画像からシーンの物理的属性
 物体形状(表面法線ベクトル) 反射特性(アルベド) 光源分布(照明マップ)  を推定することを目的としている．
Why it is worthy researching 屋内シーンの単一画像を，Inverse Rendering Network (IRN)を用いて
 アルベド 表面法線ベクトル 照明の環境マップ  の3つの属性に分解する．
今までの手法では，主に単一のオブジェクトに対して，またはシーン属性の１つのみを解決するものだった．
本稿では，屋内シーンの単一画像に対してそれらのシーン属性を同時に解くことができる事ができる．
また，SUNCG-PBRという名のデータセットを作成している．
このデータセットは以前のデータセットを大幅に改善したもの
 鏡面反射を仮定したシーン 拡散反射を仮定したシーン ground truth depth surface normals albedo Phong model parameters semantic segmentation glossiness segmentation  以前のデータセットと比べてより写実的でノイズが少ないのが特徴" />
<meta property="og:url" content="https://blog.5ebec.dev/posts/neural-inverse-rendering-of-an-indoor-scene-from-a-single-image/" />
<meta property="og:site_name" content="[論文読み] Neural Inverse Rendering of an Indoor Scene From a Single Image" />

<meta property="article:published_time" content="2019-05-17 06:52:00 &#43;0000 UTC" />



<meta name="twitter:card" content="summary" />
<meta property="og:image" content="https://blog.5ebec.dev/img/favicon/favicon.ico">



<meta name="twitter:site" content="https://blog.5ebec.dev/" />
<meta name="twitter:creator" content="@" />











</head>
<body class="">


<div class="container center">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/posts">
  <div class="logo">
    #a5ebec
  </div>
</a>

    </div>
    <div class="menu-trigger">menu</div>
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/posts">Home</a></li>
        
      
        
          <li><a href="/tags">Tags</a></li>
        
      
        
          <li><a href="/about">About</a></li>
        
      
      
    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/posts">Home</a></li>
      
    
      
        <li><a href="/tags">Tags</a></li>
      
    
      
        <li><a href="/about">About</a></li>
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://blog.5ebec.dev/posts/neural-inverse-rendering-of-an-indoor-scene-from-a-single-image/">[論文読み] Neural Inverse Rendering of an Indoor Scene From a Single Image</a></h1>
  <div class="post-meta">
    <span class="post-date">
      2019-05-17
    </span>
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://blog.5ebec.dev/tags/paper/">paper</a>&nbsp;
    
    #<a href="https://blog.5ebec.dev/tags/cv/">cv</a>&nbsp;
    
    #<a href="https://blog.5ebec.dev/tags/inverserendering/">inverserendering</a>&nbsp;
    
  </span>
  

  

  <div class="post-content">
    <ul>
<li>Neural Inverse Rendering of an Indoor Scene from a Single Image</li>
<li>Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, Jan Kautz</li>
<li>CVPR, 2019</li>
<li><a href="https://arxiv.org/abs/1901.02453v2">arXiv</a>, <a href="https://www.semanticscholar.org/paper/Neural-Inverse-Rendering-of-an-Indoor-Scene-from-a-Sengupta-Gu/f78e5da29363342ebf04d011c4f756ed021a1a11">SemanticScholar</a></li>
</ul>

<h2 id="what-it-is-about">What it is about</h2>

<p>単一画像からの屋内シーンのニューラルインバースレンダリング</p>

<h4 id="インバースレンダリングとは">インバースレンダリングとは</h4>

<p>画像からシーンの物理的属性</p>

<ul>
<li>物体形状(表面法線ベクトル)</li>
<li>反射特性(アルベド)</li>
<li>光源分布(照明マップ)</li>
</ul>

<p>を推定することを目的としている．</p>

<h2 id="why-it-is-worthy-researching">Why it is worthy researching</h2>

<p>屋内シーンの単一画像を，Inverse Rendering Network (IRN)を用いて</p>

<ul>
<li>アルベド</li>
<li>表面法線ベクトル</li>
<li>照明の環境マップ</li>
</ul>

<p>の3つの属性に分解する．<br>
今までの手法では，主に単一のオブジェクトに対して，またはシーン属性の１つのみを解決するものだった．<br>
本稿では，屋内シーンの単一画像に対してそれらのシーン属性を同時に解くことができる事ができる．</p>

<p>また，SUNCG-PBRという名のデータセットを作成している．<br>
このデータセットは以前のデータセットを大幅に改善したもの</p>

<ul>
<li>鏡面反射を仮定したシーン</li>
<li>拡散反射を仮定したシーン</li>
<li>ground truth depth</li>
<li>surface normals</li>
<li>albedo</li>
<li>Phong model parameters</li>
<li>semantic segmentation</li>
<li>glossiness segmentation</li>
</ul>

<p>以前のデータセットと比べてより写実的でノイズが少ないのが特徴</p>

<h2 id="key-idea">Key idea</h2>

<p>ラベル無しのデータから，self-supervised reconstruction loss という損失関数を使用して学習することが本稿のキーアイデア．<br>
Self-supervised Learning の Residual Appearance Renderer (RAR)によって可能としている．</p>

<h4 id="selfsupervised-learning">Self-supervised Learning</h4>

<p>自己教師あり学習．教師なし学習の一つ．<br>
pretext tasks (関係なさそうなタスク) を学習することにより，本当に学習したいタスクで使える特徴表現を学習する．</p>

<h4 id="selfsupervised-reconstruction-loss">self-supervised reconstruction loss</h4>

<p>I:元画像，A:アルベド，L:環境マップ，N:法線</p>

<p><span  class="math">\[
IRM: h_d(I;\Theta_d) \to \left\{ \hat{A}, \hat{N}, \hat{L} \right\}
\]</span></p>

<p><span  class="math">\[
Direct Renderer: f_d( \hat{A}, \hat{N}, \hat{L}) \to \hat{I_d}
\]</span></p>

<p><span  class="math">\[
RAR: f_r(I, \hat{A}, \hat{N}; \Theta_r) \to \hat{I_r}
\]</span></p>

<p>以下の式が self-supervised reconstruction loss</p>

<p><span  class="math">\[
L_u = ||I - (\hat{I_d}+\hat{I_r})||_{1}
\]</span></p>

<h2 id="how-it-is-validated-experimental-setup-and-results">How it is validated (experimental setup and results)</h2>

<h4 id="他の論文との比較">他の論文との比較</h4>

<p>より正確な法線と陰影．<br>
反射率の曖昧さを解消している．<br>
これは deep CNN を使用しているため．</p>

<p>IIWをテストセットとして比較<br>
WHDR (Weighted Human Disagreement Rate) を評価して，優れていることが確認できる.</p>

<p>アルベド，法線ベクトル，環境マップ(合成データ，実データ)全てで以前の研究より勝っている．</p>

<h2 id="limitations">Limitations</h2>

<p>単一オブジェクトに対するものではないが，シーンは屋内シーンに限定されている．</p>

<h2 id="what-you-thought">What you thought</h2>

<p>要するに，データセットを向上させて既知の学習方法を色々組み合わせたら今までより精度が上がった．というだけに聞こえるが，そういうものなのか?</p>

<h2 id="papers-to-read-before-and-after-the-work">Papers to read before and after the work</h2>

<h4 id="この論文を引用している論文">この論文を引用している論文</h4>

<ul>
<li>CVPR2019: <a href="https://arxiv.org/abs/1903.05690">Putting Humans in a Scene: Learning Affordance in 3D Indoor Environments</a></li>
</ul>

<h4 id="参考文献">参考文献</h4>

<ul>
<li>ECCV2018: <a href="https://arxiv.org/abs/1808.08601">CGIntrinsics: Better Intrinsic Image Decomposition Through Physically-Based Rendering</a></li>
<li>CVPR2018: <a href="https://www.semanticscholar.org/paper/SfSNet%3A-Learning-Shape%2C-Reflectance-and-Illuminance-Sengupta-Kanazawa/074619ffc19894c13974321d4b31144acc212f91">SfSNet: Learning Shape, Reflectance and Illuminance of Faces 'in the Wild'</a></li>
<li>CVPR2017: <a href="https://www.semanticscholar.org/paper/Physically-Based-Rendering-for-Indoor-Scene-Using-Zhang-Song/5b8d3a05d6f25158fff84bc4ef64fd12d92abc2f">Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks</a></li>
<li>CVPR2017: <a href="https://www.semanticscholar.org/paper/Semantic-Scene-Completion-from-a-Single-Depth-Image-Song-Yu/8a05db7a75c65ee61c3ca7a6e5401b946166290d">Semantic Scene Completion from a Single Depth Image</a></li>
</ul>

  </div>
  
  <div class="pagination">
    <div class="pagination__title">
      <span
        class="pagination__title-h">Read other posts</span>
      <hr />
    </div>
    <div class="pagination__buttons">
      
      <span class="button previous">
        <a href="https://blog.5ebec.dev/posts/windows-10-%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE-terminal/">
          <span class="button__icon">←</span>
          <span class="button__text">Windows 10 のための Terminal</span>
        </a>
      </span>
      
      
    </div>
  </div>
  

  

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>&copy; 2019 <a href="https://github.com/5ebec">5ebec</a></span>
        <span>:: Powered by <a href="https://gohugo.io">Hugo</a></span>
      </div>
    
  </div>
</footer>

<script src="https://blog.5ebec.dev/assets/main.js"></script>
<script src="https://blog.5ebec.dev/assets/prism.js"></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
</script>
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>


  
</div>

</body>
</html>
